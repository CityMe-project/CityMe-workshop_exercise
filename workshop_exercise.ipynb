{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "553fbade",
   "metadata": {},
   "source": [
    "## Mapping historic neighborhoods in Lisbon\n",
    "### Digital sketches and online geo-tagged activity\n",
    "\n",
    "#### Urban places and regions in GIScience â€“ concepts, methods and challenges\n",
    "AGILE 2023, Delft, Netherlands\n",
    "\n",
    "CityMe project - https://cityme.novaims.unl.pt/ \n",
    "\n",
    "FCT - Portuguese national funding agency for science, research and technology EXPL/GES-URB/1429/2021\n",
    "\n",
    "<img src=https://zap.aeiou.pt/wp-content/uploads/2015/07/5a716b2f095a5b4f0b198a53ac7cbef4.jpeg width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed01bb7",
   "metadata": {},
   "source": [
    "#### We are usingg data collected from our survey as well as geo-tagged user-generated content collected for the city of Lisbon\n",
    "We will use these data sources to explore the perceived extents of the famous historic neighborhoods of Alfama, Mouraria and Bairro Alto. These neighborhoods are part of Lisbon's history, identity and culture, but as any vernacular historic neighborhoods, boundaries might change according to individuals, mapping purposes as well as local communities.\n",
    "\n",
    "Our aim is to use spatial representations and visualizations that might help us in having a better spatial understanding regarding the neighborhoods, their cores, how their boundaries differ and the differences not only between digital surveyed data and online sources, but also between different online sources\n",
    "\n",
    "Let's start by importing our required dependencies for the exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e0970c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import shapely\n",
    "from shapely.wkt import loads\n",
    "from shapely.geometry import Polygon\n",
    "from scipy.spatial import distance\n",
    "import contextily as cx\n",
    "from contextily.tile import warp_img_transform, warp_tiles, _warper\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib_scalebar.scalebar import ScaleBar\n",
    "from libpysal.cg.alpha_shapes import alpha_shape_auto\n",
    "from esda.adbscan import ADBSCAN, get_cluster_boundary, remap_lbls\n",
    "import numba\n",
    "from shapely.ops import nearest_points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00913670",
   "metadata": {},
   "source": [
    "#### Now let's import the data.\n",
    "\n",
    "The UGC data consists of csv. files of retrieved (APIs and online resources) activity from Flickr, AirBnB, Twitter and Instagram with coordinates. \n",
    "\n",
    "The survey data correspond to shapefiles for each historic neighborhood containing the collected digital sketches\n",
    "\n",
    "Additionally, we will also need to upload Lisbon's city boundaries to filter out instances that are outside the city.\n",
    "\n",
    "Here, we upload the data, make sure that they all have the same UTM projection and transform csv. data into geodataframes with the corresponding lat/lon information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d605f691",
   "metadata": {},
   "outputs": [],
   "source": [
    "#city boundaries shapefile\n",
    "lisbon = gpd.read_file('./shapefiles/Limite_Cartografia.shp')\n",
    "lisbon = lisbon.to_crs('EPSG:27429')\n",
    "\n",
    "#survey_responses shapefiles\n",
    "alfama_survey = gpd.read_file('./shapefiles/alfama_survey.shp')\n",
    "mouraria_survey = gpd.read_file('./shapefiles/mouraria_survey.shp')\n",
    "bairro_survey = gpd.read_file('./shapefiles/bairro_survey.shp')\n",
    "\n",
    "#online sources csv. files\n",
    "twitter = pd.read_csv(\"twitter.csv\")\n",
    "instagram = pd.read_csv(\"instagram.csv\", low_memory=False)\n",
    "flickr = pd.read_csv(\"flickr.csv\")\n",
    "airbnb = pd.read_csv(\"airbnb.csv\")\n",
    "\n",
    "#turning pandas dataframes into geopandas geodataframes\n",
    "#Twitter\n",
    "twitter['geometry_real'] = twitter['geometry'].apply(lambda x: loads(x))\n",
    "twitter_gdf = gpd.GeoDataFrame(twitter, geometry= twitter.geometry_real, crs='EPSG:4326')\n",
    "twitter_gdf = twitter_gdf.to_crs('EPSG:27429')\n",
    "\n",
    "#Instagram\n",
    "instagram_gdf = gpd.GeoDataFrame(instagram, geometry=gpd.points_from_xy(instagram.lng, instagram.lat), crs='EPSG:4326')\n",
    "instagram_gdf = instagram_gdf.to_crs('EPSG:27429')\n",
    "\n",
    "#Flickr\n",
    "flickr_gdf = gpd.GeoDataFrame(flickr, geometry=gpd.points_from_xy(flickr.lng, flickr.lat), crs='EPSG:4326')\n",
    "flickr_gdf = flickr_gdf.to_crs('EPSG:27429')\n",
    "\n",
    "#Airbnb\n",
    "airbnb_gdf = gpd.GeoDataFrame(airbnb, geometry=gpd.points_from_xy(airbnb.longitude, airbnb.latitude), crs='EPSG:4326')\n",
    "airbnb_gdf = airbnb_gdf.to_crs('EPSG:27429')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493294c3",
   "metadata": {},
   "source": [
    "##### Now let's clip the online sources to Lisbon's boundaries and take a better look at them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1db545-e957-4a8c-847b-9eef3af1bcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clipping sources to Lisbon's city boundaries using geopandas clip\n",
    "twitter = gpd.clip(twitter_gdf, lisbon.geometry)\n",
    "instagram = gpd.clip(instagram_gdf, lisbon.geometry)\n",
    "flickr = gpd.clip(flickr_gdf, lisbon.geometry)\n",
    "airbnb = gpd.clip(airbnb_gdf, lisbon.geometry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6022d726",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taking a peek at the geodataframes and their size\n",
    "#Twitter\n",
    "print(twitter.shape[0])\n",
    "twitter.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbfa0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instagram\n",
    "print(instagram.shape[0])\n",
    "instagram.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4500c20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Flickr\n",
    "print(flickr.shape[0])\n",
    "flickr.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c9b978c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Airbnb\n",
    "print(airbnb.shape[0])\n",
    "airbnb.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b183b29a",
   "metadata": {},
   "source": [
    "##### Now let's check their unfiltered distribution onto the basemap of Lisbon\n",
    "\n",
    "Feel free to change colors, transparencies, basemap, markersizes or any other visualization feature throughout the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e1be7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting all UGC data \n",
    "\n",
    "#Figure size\n",
    "plt.rcParams['figure.figsize'] = [6, 6] \n",
    "\n",
    "#Subplots\n",
    "fig, myax = plt.subplots()\n",
    "\n",
    "#Plotting and defining markersize, color and transparency\n",
    "flickr.plot(ax=myax,markersize=10, color='mediumpurple', alpha=0.6)\n",
    "airbnb.plot(ax=myax,markersize=10, color='plum', alpha=0.6)\n",
    "twitter.plot(ax=myax,markersize=10, color='steelblue', alpha=0.6)\n",
    "instagram.plot(ax=myax,markersize=10, color='firebrick', alpha=0.6)\n",
    "\n",
    "#Adding base-map with contextily\n",
    "cx.add_basemap(myax, crs=twitter.crs.to_string(), source=cx.providers.CartoDB.Positron, zoom=15,attribution_size=0)\n",
    "\n",
    "#Removing axis, adding scale bar and title\n",
    "myax.axis(\"off\")\n",
    "myax.add_artist(ScaleBar(1, location='upper left'))\n",
    "myax.set_title('UGC')\n",
    "\n",
    "#Defining personalized legends\n",
    "twitter_leg = plt.scatter([],[], marker='o',color='steelblue', label='Twitter')\n",
    "instagram_leg = plt.scatter([],[], marker='o',color='firebrick', label='Instagram')\n",
    "flickr_leg = plt.scatter([],[], marker='o',color='mediumpurple', label='Flickr')\n",
    "airbnb_leg = plt.scatter([],[], marker='o',color='plum', label='AirBnB')\n",
    "\n",
    "#Adding legend\n",
    "plt.legend(handles=[twitter_leg, instagram_leg, flickr_leg, airbnb_leg], loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7737fb6",
   "metadata": {},
   "source": [
    "#### Now we are going to filter instances where there are mentions to the neighborhoods\n",
    "Columns with textual data generated/filled by users and metadata for each source and neighborhood\n",
    "\n",
    "##### While Twitter, Instagram and Flickr we are using more fields, we opt to only use the name field in Airbnb, why is that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a363b101",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alfama\n",
    "#Twitter and columns\n",
    "twitter_alfama = twitter[twitter['text'].str.contains(\"Alfama|alfama\") | twitter['full_name'].str.contains(\"Alfama|alfama\") | twitter['name'].str.contains(\"Alfama|alfama\")]\n",
    "\n",
    "#Instagram and columns\n",
    "instagram_alfama = instagram[instagram['caption_text'].str.contains(\"Alfama|alfama\") | instagram['name'].str.contains(\"Alfama|alfama\") | instagram['address'].str.contains(\"Alfama|alfama\")]\n",
    "\n",
    "#Flickr and columns\n",
    "flickr_alfama = flickr[flickr['title'].str.contains(\"Alfama|alfama\") | flickr['tags'].str.contains(\"Alfama|alfama\") | flickr['neighbourhood'].str.contains(\"Alfama|alfama\") | flickr['locality'].str.contains(\"Alfama|alfama\") | flickr['desc'].str.contains(\"Alfama|alfama\") | flickr['county'].str.contains(\"Alfama|alfama\") | flickr['region'].str.contains(\"Alfama|alfama\")]\n",
    "\n",
    "#Airbnb and columns\n",
    "airbnb_alfama = airbnb[airbnb['name'].str.contains(\"Alfama|alfama\", na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20eda95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mouraria\n",
    "#Twitter and columns\n",
    "twitter_mouraria = twitter[twitter['text'].str.contains(\"Mouraria|mouraria\") | twitter['full_name'].str.contains(\"Mouraria|mouraria\") | twitter['name'].str.contains(\"Mouraria|mouraria\")]\n",
    "\n",
    "#Instagram and columns\n",
    "instagram_mouraria = instagram[instagram['caption_text'].str.contains(\"Mouraria|mouraria\") | instagram['name'].str.contains(\"Mouraria|mouraria\") | instagram['address'].str.contains(\"Mouraria|mouraria\")]\n",
    "\n",
    "#Flickr and columns\n",
    "flickr_mouraria = flickr[flickr['title'].str.contains(\"Mouraria|mouraria\") | flickr['tags'].str.contains(\"Mouraria|mouraria\") | flickr['neighbourhood'].str.contains(\"Mouraria|mouraria\") | flickr['locality'].str.contains(\"Mouraria|mouraria\") | flickr['desc'].str.contains(\"Mouraria|mouraria\") | flickr['county'].str.contains(\"Mouraria|mouraria\") | flickr['region'].str.contains(\"Mouraria|mouraria\")]\n",
    "\n",
    "#Airbnb and columns\n",
    "airbnb_mouraria = airbnb[airbnb['name'].str.contains(\"Mouraria|mouraria\", na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3207098",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bairro Alto\n",
    "#Twitter and columns\n",
    "twitter_bairro = twitter[twitter['text'].str.contains(\"Bairro Alto|bairro alto\") | twitter['full_name'].str.contains(\"Bairro Alto|bairro alto\") | twitter['name'].str.contains(\"Bairro Alto|bairro alto\")]\n",
    "\n",
    "#Instagram and columns\n",
    "instagram_bairro = instagram[instagram['caption_text'].str.contains(\"Bairro Alto|bairro alto\") | instagram['name'].str.contains(\"Bairro Alto|bairro alto\") | instagram['address'].str.contains(\"Bairro Alto|bairro alto\")]\n",
    "\n",
    "#Flickr and columns\n",
    "flickr_bairro = flickr[flickr['title'].str.contains(\"Bairro Alto|bairro alto\") | flickr['tags'].str.contains(\"Bairro Alto|bairro alto\") | flickr['neighbourhood'].str.contains(\"Bairro Alto|bairro alto\") | flickr['locality'].str.contains(\"Bairro Alto|bairro alto\") | flickr['desc'].str.contains(\"Bairro Alto|bairro alto\") | flickr['county'].str.contains(\"Bairro Alto|bairro alto\") | flickr['region'].str.contains(\"Bairro Alto|bairro alto\")]\n",
    "\n",
    "#Airbnb and columns\n",
    "airbnb_bairro = airbnb[airbnb['name'].str.contains(\"Bairro Alto|bairro alto\", na=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53aa8c9c",
   "metadata": {},
   "source": [
    "#### Let's check the distribution of UGC data mentions to neighborhoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c28018",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alfama\n",
    "\n",
    "#Figure size\n",
    "plt.rcParams['figure.figsize'] = [6, 6] \n",
    "\n",
    "#Subplots\n",
    "fig, myax = plt.subplots()\n",
    "\n",
    "#Plotting Alfama UGC data and defining markersize, color and transparency\n",
    "flickr_alfama.plot(ax=myax,markersize=10, color='mediumpurple', alpha=0.6)\n",
    "airbnb_alfama.plot(ax=myax,markersize=10, color='plum', alpha=0.6)\n",
    "instagram_alfama.plot(ax=myax,markersize=10, color='firebrick', alpha=0.6)\n",
    "twitter_alfama.plot(ax=myax,markersize=10, color='steelblue', alpha=0.6)\n",
    "\n",
    "#Adding base-map with contextily\n",
    "cx.add_basemap(myax, crs=twitter.crs.to_string(), source=cx.providers.CartoDB.Positron, zoom=15,attribution_size=0)\n",
    "\n",
    "#Removing axis, adding scale bar and title\n",
    "myax.axis(\"off\")\n",
    "myax.add_artist(ScaleBar(1, location='upper left'))\n",
    "myax.set_title('Alfama')\n",
    "\n",
    "#Defining personalized legends\n",
    "twitter_leg = plt.scatter([],[], marker='o',color='steelblue', label='Twitter')\n",
    "instagram_leg = plt.scatter([],[], marker='o',color='firebrick', label='Instagram')\n",
    "flickr_leg = plt.scatter([],[], marker='o',color='mediumpurple', label='Flickr')\n",
    "airbnb_leg = plt.scatter([],[], marker='o',color='plum', label='AirBnB')\n",
    "\n",
    "#Adding legends\n",
    "plt.legend(handles=[twitter_leg, instagram_leg, flickr_leg, airbnb_leg], loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4726e5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mouraria\n",
    "\n",
    "#Figure size\n",
    "plt.rcParams['figure.figsize'] = [6, 6] \n",
    "\n",
    "#Subplots\n",
    "fig, myax = plt.subplots()\n",
    "\n",
    "#Plotting Alfama UGC data and defining markersize, color and transparency\n",
    "flickr_mouraria.plot(ax=myax,markersize=10, color='mediumpurple', alpha=0.6)\n",
    "airbnb_mouraria.plot(ax=myax,markersize=10, color='plum', alpha=0.6)\n",
    "instagram_mouraria.plot(ax=myax,markersize=10, color='firebrick', alpha=0.6)\n",
    "twitter_mouraria.plot(ax=myax,markersize=10, color='steelblue', alpha=0.6)\n",
    "\n",
    "#Adding base-map with contextily\n",
    "cx.add_basemap(myax, crs=twitter.crs.to_string(), source=cx.providers.CartoDB.Positron, zoom=15,attribution_size=0)\n",
    "\n",
    "#Removing axis, adding scale bar and title\n",
    "myax.axis(\"off\")\n",
    "myax.add_artist(ScaleBar(1, location='upper left'))\n",
    "myax.set_title('Mouraria')\n",
    "\n",
    "#Defining personalized legends\n",
    "twitter_leg = plt.scatter([],[], marker='o',color='steelblue', label='Twitter')\n",
    "instagram_leg = plt.scatter([],[], marker='o',color='firebrick', label='Instagram')\n",
    "flickr_leg = plt.scatter([],[], marker='o',color='mediumpurple', label='Flickr')\n",
    "airbnb_leg = plt.scatter([],[], marker='o',color='plum', label='AirBnB')\n",
    "\n",
    "#Adding legends\n",
    "plt.legend(handles=[twitter_leg, instagram_leg, flickr_leg, airbnb_leg], loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc52b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bairro Alto\n",
    "\n",
    "#Figure size\n",
    "plt.rcParams['figure.figsize'] = [6, 6] \n",
    "\n",
    "#Subplots\n",
    "fig, myax = plt.subplots()\n",
    "\n",
    "#Plotting Alfama UGC data and defining markersize, color and transparency\n",
    "flickr_bairro.plot(ax=myax,markersize=10, color='mediumpurple', alpha=0.6)\n",
    "airbnb_bairro.plot(ax=myax,markersize=10, color='plum', alpha=0.6)\n",
    "instagram_bairro.plot(ax=myax,markersize=10, color='firebrick', alpha=0.6)\n",
    "twitter_bairro.plot(ax=myax,markersize=10, color='steelblue', alpha=0.6)\n",
    "\n",
    "#Adding base-map with contextily\n",
    "cx.add_basemap(myax, crs=twitter.crs.to_string(), source=cx.providers.CartoDB.Positron, zoom=15,attribution_size=0)\n",
    "\n",
    "#Removing axis, adding scale bar and title\n",
    "myax.axis(\"off\")\n",
    "myax.add_artist(ScaleBar(1, location='upper left'))\n",
    "myax.set_title('Bairro Alto')\n",
    "\n",
    "#Defining personalized legends\n",
    "twitter_leg = plt.scatter([],[], marker='o',color='steelblue', label='Twitter')\n",
    "instagram_leg = plt.scatter([],[], marker='o',color='firebrick', label='Instagram')\n",
    "flickr_leg = plt.scatter([],[], marker='o',color='mediumpurple', label='Flickr')\n",
    "airbnb_leg = plt.scatter([],[], marker='o',color='plum', label='AirBnB')\n",
    "\n",
    "#Adding legends\n",
    "plt.legend(handles=[twitter_leg, instagram_leg, flickr_leg, airbnb_leg], loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b357f57e",
   "metadata": {},
   "source": [
    "### Creating shapes from geo-tagged activity using A-DBSCAN\n",
    "#### Carrying out density-based clustering and retrieving alpha-shapes for each source and neighborhood\n",
    "\n",
    "##### Source:\n",
    "https://pysal.org/esda/notebooks/adbscan_berlin_example.html\n",
    "\n",
    "We will need to set the EPS and the minimum samples parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60d37cc",
   "metadata": {},
   "source": [
    "### Alfama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d097a405",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding our geodataframes to a list\n",
    "alfama_list = [twitter_alfama, instagram_alfama, flickr_alfama, airbnb_alfama]\n",
    "\n",
    "#Now, we will perform a nearest neighbor analysis and obtain each distribution's median\n",
    "\n",
    "# Create an empty list to store the results\n",
    "eps_results = []\n",
    "\n",
    "# Iterate over the geodataframes list for Alfama\n",
    "for gdf in alfama_list:\n",
    "\n",
    "    # Remove duplicated geometries\n",
    "    gdf = gdf.drop_duplicates(subset='geometry')\n",
    "\n",
    "    # Get the coordinates from the GeoDataFrame's geometry column\n",
    "    coordinates = np.array(list(gdf.geometry.apply(lambda geom: (geom.x, geom.y))))\n",
    "\n",
    "    # Compute the distance matrix\n",
    "    dist_matrix = distance.cdist(coordinates, coordinates)\n",
    "\n",
    "    # Flatten the distance matrix\n",
    "    flattened_dist = dist_matrix.flatten()\n",
    "    \n",
    "    # Calculate the median\n",
    "    medians = np.percentile(flattened_dist, 50)\n",
    "\n",
    "    # Append the result to the list\n",
    "    eps_results.append(medians)\n",
    "    \n",
    "# These are the distances we will use as our eps parameters for each clustering    \n",
    "print(eps_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6c0c79",
   "metadata": {},
   "source": [
    "#### The following workflow applies the A-DBSCAN algorithm based on the distances we retrieved and set the minimum samples as half of our datasets\n",
    "##### Running a sensitivity analysis is out of scope here, but feel free to change parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ae3364",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To apply the algorithm we need to get X and Y columns corresponding to long and lat values\n",
    "\n",
    "#Iterating through the alfama datasets for each source and adding X and Y columns\n",
    "for gdf in alfama_list:\n",
    "    # Extract the longitude and latitude from the geometry column\n",
    "    gdf['X'] = gdf.geometry.x.values\n",
    "    gdf['Y'] = gdf.geometry.y.values\n",
    "    \n",
    "    \n",
    "# Now let's apply the A-DBSCAN for each geodataframe using our eps_results distance\n",
    "# and half of instances as our minimum samples to form a cluster\n",
    "\n",
    "# Create an empty list to store the ADBSCAN models\n",
    "adbs_list = []\n",
    "\n",
    "# Iterate over the GeoDataFrames and eps values\n",
    "for gdf, eps in zip(alfama_list, eps_results):\n",
    "    \n",
    "    #Remove duplicated geometries\n",
    "    gdf = gdf.drop_duplicates(subset='geometry')\n",
    "    \n",
    "    # Calculate min_samples based on the current GeoDataFrame\n",
    "    # Half of the count of instances\n",
    "    min_samples = int(len(gdf)*0.50)\n",
    "\n",
    "    # Create ADBSCAN object\n",
    "    adbs = ADBSCAN(eps, min_samples, pct_exact=0.5, reps=50, keep_solus=True)\n",
    "    \n",
    "    # Set random seed for replication\n",
    "    np.random.seed(1234)\n",
    "\n",
    "    # Fit ADBSCAN on the current GeoDataFrame\n",
    "    adbs.fit(gdf)\n",
    "    \n",
    "    # Append the fitted ADBSCAN model to the adbs_list\n",
    "    adbs_list.append(adbs)\n",
    "    \n",
    "# Create an empty list to store the cluster boundaries\n",
    "boundaries = []\n",
    "\n",
    "# Iterate over the ADBSCAN models\n",
    "for adbs, gdf in zip(adbs_list, alfama_list):\n",
    "    \n",
    "    # Get the cluster boundaries\n",
    "    shape = get_cluster_boundary(adbs.votes[\"lbls\"], gdf, ['X', 'Y'], crs=gdf.crs)\n",
    "\n",
    "    # Append the cluster boundaries to the list\n",
    "    boundaries.append(shape)\n",
    "    \n",
    "# List to store the transformed cluster boundaries to GeoDataFrames\n",
    "boundary_gdfs = []\n",
    "\n",
    "# Iterate over the boundaries\n",
    "for boundary in boundaries:\n",
    "    \n",
    "    # Create a GeoDataFrame from the Polygon geometry\n",
    "    gdf = gpd.GeoDataFrame(geometry=boundary, crs=twitter.crs)\n",
    "\n",
    "    # Append the GeoDataFrame to the boundary_gdfs list\n",
    "    boundary_gdfs.append(gdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cb9ef1",
   "metadata": {},
   "source": [
    "#### Let's check the resulting shapes from our clustering for each source representing the Alfama neighborhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bbbd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Figure size\n",
    "plt.rcParams['figure.figsize'] = [6, 6] \n",
    "\n",
    "#Subplots\n",
    "fig, myax = plt.subplots()\n",
    "\n",
    "#Plotting boundaries, defining edgecolors and linewidths\n",
    "myax = boundary_gdfs[0].plot(ax=myax, edgecolor=\"steelblue\", facecolor=\"none\", linewidth=1.5)\n",
    "boundary_gdfs[1].plot(ax=myax, edgecolor=\"firebrick\", facecolor=\"none\", linewidth=1.5)\n",
    "boundary_gdfs[2].plot(ax=myax, edgecolor=\"mediumpurple\", facecolor=\"none\", linewidth=1.5)\n",
    "boundary_gdfs[3].plot(ax=myax, edgecolor=\"plum\", facecolor=\"none\", linewidth=1.5)\n",
    "\n",
    "#Adding base-map with contextily\n",
    "cx.add_basemap(ax=myax, crs=twitter_alfama.crs.to_string(), attribution_size=0, zoom=15)\n",
    "\n",
    "#Removing axis, adding title and scale bar\n",
    "myax.axis(\"off\");\n",
    "myax.add_artist(ScaleBar(1, location='upper left'))\n",
    "myax.set_title('A-DBSCAN shapes')\n",
    "\n",
    "\n",
    "#Defining personalized legends\n",
    "twitter = plt.scatter([],[], marker='o',color='steelblue', label='Twitter')\n",
    "instagram = plt.scatter([],[], marker='o',color='firebrick', label='Instagram')\n",
    "flickr = plt.scatter([],[], marker='o',color='mediumpurple', label='Flickr')\n",
    "airbnb = plt.scatter([],[], marker='o',color='plum', label='AirBnb')\n",
    "\n",
    "#Adding legends\n",
    "plt.legend(handles=[twitter, instagram, flickr, airbnb], loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddc1c10",
   "metadata": {},
   "source": [
    "#### Now let's check the shapes with some color overlay to get a better perspective on the spatial overlap between sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfea743e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Figure size\n",
    "plt.rcParams['figure.figsize'] = [6, 6] \n",
    "\n",
    "#Subplots\n",
    "fig, myax = plt.subplots()\n",
    "\n",
    "#Plotting the data with defined colors from the YlOrRd matplotlib color map and setting transparencies\n",
    "myax = boundary_gdfs[1].plot(ax=myax, facecolor=\"#ffffcc\", edgecolor=\"none\", alpha=0.6)\n",
    "boundary_gdfs[2].plot(ax=myax, facecolor=\"#febf5a\", edgecolor=\"none\", alpha=0.4)\n",
    "boundary_gdfs[3].plot(ax=myax, facecolor=\"#f43d25\", edgecolor=\"none\", alpha=0.4)\n",
    "boundary_gdfs[0].plot(ax=myax, facecolor=\"#800026\", edgecolor=\"none\", alpha=0.4)\n",
    "\n",
    "#Adding base-map with contextily\n",
    "cx.add_basemap(ax=myax, crs=twitter_alfama.crs.to_string(), attribution_size=0, zoom=15)\n",
    "\n",
    "#Removing axis, adding scale bar and title\n",
    "myax.axis(\"off\");\n",
    "myax.add_artist(ScaleBar(1, location='upper left'))\n",
    "myax.set_title('A-DBSCAN shapes')\n",
    "\n",
    "#Defining personalized legends\n",
    "twitter = plt.scatter([],[], marker='o',color='#800026', label='Twitter')\n",
    "instagram = plt.scatter([],[], marker='o',color='#ffffcc', label='Instagram')\n",
    "flickr = plt.scatter([],[], marker='o',color='#febf5a', label='Flickr')\n",
    "airbnb = plt.scatter([],[], marker='o',color='#f43d25', label='AirBnb')\n",
    "\n",
    "#Adding legends\n",
    "plt.legend(handles=[twitter, instagram, flickr, airbnb], loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9442308c",
   "metadata": {},
   "source": [
    "### Let's check the survey responses for Alfama\n",
    "##### We will remove invalid geometries, check the data and remove polygons that are objectively far off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062fd653",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, let's check how many geometries are invalid\n",
    "print(alfama_survey.geometry.is_valid.value_counts())\n",
    "\n",
    "#Then, let's remove invalid geometries\n",
    "alfama_survey = alfama_survey.loc[alfama_survey['geometry'].is_valid, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a78d64",
   "metadata": {},
   "source": [
    "#### Now let's check the polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce45009f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Figure size\n",
    "plt.rcParams['figure.figsize'] = [6, 6] \n",
    "\n",
    "#Subplots\n",
    "fig, myax = plt.subplots()\n",
    "\n",
    "#Adding alfama survey polygons, defining color and transparency\n",
    "alfama_survey.plot(ax=myax,markersize=20, color='indianred', alpha=0.3, edgecolor=\"white\")\n",
    "\n",
    "#Adding base-map with contextily\n",
    "cx.add_basemap(myax, crs=alfama_survey.crs.to_string(), source=cx.providers.CartoDB.Positron, attribution_size=0)\n",
    "\n",
    "#Removing axis, adding scale bar and title\n",
    "myax.axis(\"off\")\n",
    "myax.add_artist(ScaleBar(1, location='upper left'))\n",
    "myax.set_title('Alfama survey responses')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485c04d6",
   "metadata": {},
   "source": [
    "#### It seems there were some \"errors\" during data collection, let's remove these polygons by first obtaining the polygons' centroids, then performing a nearest neighbor analysis and obtaining their distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec469f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtaining centroids\n",
    "centroids_alfama = gpd.GeoDataFrame(alfama_survey.geometry.centroid, geometry = alfama_survey.geometry.centroid)\n",
    "\n",
    "#Obtaining the nearest neighbors within the same geodataframe\n",
    "#Column foor storing the nearest geometry\n",
    "centroids_alfama['nearest_geometry'] = '' \n",
    "\n",
    "#Iterating through index and rows of the geodataframe\n",
    "for index, row in centroids_alfama.iterrows():\n",
    "    point = row.geometry\n",
    "    multipoint = centroids_alfama.drop(index, axis=0).geometry.unary_union\n",
    "    queried_geom, nearest_geom = nearest_points(point, multipoint)\n",
    "    centroids_alfama.loc[index, 'nearest_geometry'] = nearest_geom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2504e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing polygons that are too far from others\n",
    "\n",
    "#obtaining the nearest distance distances for each centroid\n",
    "nearest_centroids_alfama = gpd.GeoDataFrame(centroids_alfama.nearest_geometry, geometry = centroids_alfama.nearest_geometry)\n",
    "nearest_centroids_alfama.set_crs(epsg = 32629, inplace = True)\n",
    "nearest_centroids_alfama['distance'] = centroids_alfama.geometry.distance(nearest_centroids_alfama.geometry)\n",
    "\n",
    "#threshold for removing - 90% percentile distribution\n",
    "threshold = nearest_centroids_alfama['distance'].quantile(q=0.90)\n",
    "\n",
    "#removing outlier centroids based on the threshold\n",
    "alfama_centroids = nearest_centroids_alfama[nearest_centroids_alfama['distance'] <= threshold]\n",
    "\n",
    "#removing correspondent outlier polygons\n",
    "i1 = alfama_survey.index\n",
    "i2 = alfama_centroids.index\n",
    "alfama_survey_valid = alfama_survey[i1.isin(i2)]\n",
    "\n",
    "#visualizing output polygons\n",
    "plt.rcParams['figure.figsize'] = [6, 6] \n",
    "fig, myax = plt.subplots()\n",
    "alfama_survey_valid.plot(ax=myax,markersize=20, color='indianred', alpha=0.3, edgecolor=\"white\")\n",
    "cx.add_basemap(myax, crs=alfama_survey_valid.crs.to_string(), attribution_size=0)\n",
    "\n",
    "#setting the bounding boxes of visualization\n",
    "myax.axis(\"off\")\n",
    "myax.add_artist(ScaleBar(1, location='upper left'))\n",
    "myax.set_title('Alfama')\n",
    "print(alfama_survey_valid.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f0e2c9",
   "metadata": {},
   "source": [
    "##### From 182 valid geometries, we ended up with 163 responses, now we will check their agreement or spatial overlap \n",
    "##### We first will define a function to retrieve all overlaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd55e024",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to retrieve all overlaps \n",
    "def count_overlapping_features(gdf):\n",
    "    #generating all of the split pieces\n",
    "    boundaries = gdf.geometry.exterior.unary_union\n",
    "    new_polys = list(shapely.ops.polygonize(boundaries))\n",
    "    new_gdf = gpd.GeoDataFrame(geometry=new_polys, crs=gdf.crs)\n",
    "    new_gdf['id'] = range(len(new_gdf))\n",
    "\n",
    "    #count overlapping by sjoin between pieces centroid and the input gdf \n",
    "    new_gdf_centroid = new_gdf.copy()\n",
    "    new_gdf_centroid['geometry'] = new_gdf.centroid\n",
    "    overlapcount = gpd.sjoin(new_gdf_centroid,gdf)\n",
    "    overlapcount = overlapcount.groupby(['id'])['index_right'].count().rename('overlap_count').reset_index()\n",
    "    out_gdf = pd.merge(new_gdf,overlapcount)\n",
    "    return out_gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8aa66d",
   "metadata": {},
   "source": [
    "##### Checking the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4e9e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking function\n",
    "# Plot the GeoDataFrame\n",
    "ax = count_overlapping_features(alfama_survey_valid).plot('overlap_count', cmap='viridis', edgecolor=\"none\", legend=False)\n",
    "\n",
    "# Get the minimum and maximum values for the colorbar\n",
    "min_value = count_overlapping_features(alfama_survey_valid)['overlap_count'].min()\n",
    "max_value = count_overlapping_features(alfama_survey_valid)['overlap_count'].max()\n",
    "\n",
    "# Create a new colorbar\n",
    "cbar = plt.colorbar(ax.get_children()[0], ax=ax, orientation='vertical', pad=0.1, aspect=40, shrink = 0.6)\n",
    "\n",
    "# Set the colorbar ticks and tick labels\n",
    "cbar.set_ticks([min_value, max_value])\n",
    "cbar.set_ticklabels([min_value, max_value])\n",
    "\n",
    "# Adjust the colorbar position\n",
    "cbar.ax.yaxis.set_label_position('left')\n",
    "cbar.ax.yaxis.set_ticks_position('left')\n",
    "\n",
    "# Remove axis ticks\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3baebe",
   "metadata": {},
   "source": [
    "##### We have up until 107 overlap geometries, now we will visualize it better and see in terms of percentages onto the basemap of Lisbon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe878f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the overlap counts from the function\n",
    "#overlap gdf\n",
    "overlap_alfama_survey = count_overlapping_features(alfama_survey_valid)\n",
    "count_percentage = (overlap_alfama_survey['overlap_count'] - overlap_alfama_survey['overlap_count'].min()) / (overlap_alfama_survey['overlap_count'].max() - overlap_alfama_survey['overlap_count'].min())\n",
    "overlap_alfama_survey['count_percentage'] = count_percentage.round(decimals=2)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9130dff",
   "metadata": {},
   "source": [
    "##### Let's take a closer look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1885e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting in percentages - larger scale and removing small percentages (less than 20%)\n",
    "\n",
    "#Subplots\n",
    "fig, myax = plt.subplots()\n",
    "\n",
    "#Plotting higher than 20% overlap, setting color map, legend and transparency\n",
    "overlap_alfama_survey[overlap_alfama_survey['count_percentage'] > 20].plot('count_percentage', ax=myax, cmap='YlOrRd', alpha=0.5, edgecolor=\"none\", legend=True, legend_kwds={'shrink': 0.45})\n",
    "\n",
    "#Adding base-map with contextily\n",
    "cx.add_basemap(myax, crs=alfama_survey_valid.crs.to_string(), source=cx.providers.CartoDB.Positron, attribution_size=0, zoom=15)\n",
    "\n",
    "#Adding scale, title and removing axis\n",
    "myax.set_title('Survey agreement (>20% overlap)')\n",
    "myax.add_artist(ScaleBar(1, location='upper left'))\n",
    "myax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e8dfe8",
   "metadata": {},
   "source": [
    "### The last visualization will put side to side the agreement between participants and the agreement between sources\n",
    "\n",
    "##### What can they tell us?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc5489f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a figure with two subplots and set figure size\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "# getting the extent of the survey overlaps higher than 20% layer\n",
    "# to set the same extent for both maps\n",
    "xmin, ymin, xmax, ymax = overlap_alfama_survey[overlap_alfama_survey['count_percentage'] > 20].total_bounds\n",
    "\n",
    "# First subplot - Spatial agreement between participants\n",
    "plt.rcParams['figure.figsize'] = [8, 8]\n",
    "ax1 = overlap_alfama_survey[overlap_alfama_survey['count_percentage'] > 20].plot('count_percentage', ax=ax1, cmap='YlOrRd', alpha=0.5, edgecolor=\"none\", legend=True,legend_kwds={'shrink': 0.6})\n",
    "cx.add_basemap(ax=ax1, crs=alfama_survey_valid.crs.to_string(), attribution_size=0, zoom=15)\n",
    "ax1.axis(\"off\")\n",
    "ax1.set_title('Spatial agreement between participants')\n",
    "ax1.add_artist(ScaleBar(1, location='upper left'))\n",
    "\n",
    "# Second subplot - Spatial agreement between sources\n",
    "plt.rcParams['figure.figsize'] = [8, 8] \n",
    "ax2 = boundary_gdfs[1].plot(ax=ax2, facecolor=\"#ffffcc\", edgecolor=\"none\", alpha=0.3)\n",
    "boundary_gdfs[2].plot(ax=ax2, facecolor=\"#febf5a\", edgecolor=\"none\", alpha=0.3)\n",
    "boundary_gdfs[3].plot(ax=ax2, facecolor=\"#f43d25\", edgecolor=\"none\", alpha=0.3)\n",
    "boundary_gdfs[0].plot(ax=ax2, facecolor=\"#800026\", edgecolor=\"none\", alpha=0.3)\n",
    "ax2.set_xlim(xmin-100, xmax+100)\n",
    "ax2.set_ylim(ymin-100, ymax+100)\n",
    "ax2.axis(\"off\");\n",
    "ax2.set_title('Spatial agreement between UGC sources')\n",
    "ax2.add_artist(ScaleBar(1, location='upper left'))\n",
    "cx.add_basemap(ax=ax2, crs=twitter_alfama.crs.to_string(), attribution_size=0, zoom=15)\n",
    "\n",
    "#Defining personalized legends\n",
    "twitter = plt.scatter([],[], marker='o',color='#800026', label='Twitter')\n",
    "instagram = plt.scatter([],[], marker='o',color='#ffffcc', label='Instagram')\n",
    "flickr = plt.scatter([],[], marker='o',color='#febf5a', label='Flickr')\n",
    "airbnb = plt.scatter([],[], marker='o',color='#f43d25', label='AirBnb')\n",
    "\n",
    "#Adding legends\n",
    "plt.legend(handles=[twitter, instagram, flickr, airbnb], loc='lower right')\n",
    "\n",
    "# Show the combined plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2a617b",
   "metadata": {},
   "source": [
    "### Now, based on above code, you should try to carry out the workflow with the other neighborhoods!\n",
    "\n",
    "##### Remember to check parameters as for each source and neighborhood, representative sampling will change\n",
    "\n",
    "<img src=https://c6.quickcachr.fotos.sapo.pt/i/o6e09e224/18830918_tQzwt.jpeg width=\"400\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
